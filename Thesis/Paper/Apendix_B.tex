\chapter{Some results of Hilbert's space theory}
\label{Appendix_A}
	
	\section{Important Inequalities in Hilbert Spaces and Some about Semi-Group Theory}
	\subsection{The Cauchy-Schwarz inequality}
	\label{Cauchy_Schwarz}
		Let $X$ be a Hilbert space, endowed with the inner product $\langle u, v \rangle$ and the associated norm $\| u \|$. The Cauchy-Schwarz inequality states that
		\begin{align*}
			|\langle u, v \rangle| \leq \| u \| \| v \|, \hspace{2mm} \text{for all} \hspace{2mm} u, v \in X
		\end{align*}

	\subsection{The Poincar\'e Inequality}
	\label{Poincare}
		Let $v$ be a function of $H^1 (a, b)$. We know that $v$ is continuous on $[a, b]$. Assume that at a point $x_0 ∈ [a, b]$, $v_0 (x_0) = 0$. The Poincar\'e inequality states that there exists a constant $C$ (depending upon the interval length) such that
		\begin{align*}
			\| v \|_{L^2 [a, b]} \leq C \| v' \|_{L^2 [a, b]}
		\end{align*}
		i.e., the $L^2$-norm of the function is bounded by the $L^2$-norm of the derivative. The Poincar\'e inequality applies to functions belonging to $H^0_1 (a, b)$, where
		\begin{align*}
			H^1_0 (a, b) = \{ v \in H^1 (a, b) : v(a) = v(b) = 0 \} ,
		\end{align*}
		for which $x_0 = a$ or $b$. and also to functions of $H^1 (a, b)$ that have zero average on $(a, b)$, since necessarily such functions change sign in the domain.		

	\section{Distributions (or generalized functions) Theory}
	
	\begin{definition}
		The support of a function $f(x)$ is the lock of the set of all points $x$ such that $f(x) = 0$. We will denote the support of a $f$ function for $sop(f)$. If $sop(f)$ is a bounded set, then it is said that $f$ has compact support. 
	\end{definition}
	
	Let us denote $D_m$ as the complex function space $\varphi(x)$ defined in $\mathbb{R}^n$ with continuous partial derivatives up to order $m$ and compact support. This view is also taken as the basis for the definition of an arbitrary generalized function. Accordingly, consider the space $D$ consisting of functions at real values $\varphi(x) = \varphi(x_1, x_2, \dots, x_n)$, such that the following is satisfied:
	\begin{itemize}
		\item 
		$\varphi(x)$ is an infinitely differentiable function defined at every point of $\mathbb{R}^n$. This means that $D_{\alpha}$ exists for all multi-indexes. Such a function is also called a $C^{\infty}$ function.
		\item 
		There is a $A$ number such that $\varphi(x)$ is canceled for $r > A$. This means that $\varphi(x)$ has compact support.
	\end{itemize}
	
	\begin{teor}
		Let $U \subset R^n$ open and $f \in L^1_{loc} (U)$. Then $f$ can be identified with the distribution in U $f: D (U) \rightarrow C$ by using the formula
		\begin{align*}
			\langle f, \varphi \rangle = \displaystyle \int_{R^n} f(x) \varphi (x) dx.
		\end{align*}
		In addition, a distribution defined by this formula is called regular. If a distribution is not regular, it is called singular.
	\end{teor}
	
	Regular distributions can also be defined by partial differential operators. If $f(x) \in L^1_{loc}$, we can define a distribution as
	\begin{align*}
		\langle f, \varphi \rangle = \displaystyle \int_{R^n} f(x) D^{\alpha} \varphi (x) dx, \hspace{2mm} \varphi \in D.
	\end{align*}

	Let $f$ be an integrable function especially compact, except for those that contain certain singular points: to simplify, we will assume a single singular point, in $x_0$. The functional $f$ such that
	\begin{align*}
		\langle f, \varphi \rangle = \displaystyle \int_{R^n} f(x) \varphi (x) dx, \hspace{2mm} \varphi \in D.
	\end{align*}
	it is not a distribution over $\mathbb{R}^n$, the right side is only defined, in general, if the support excludes $x_0$; In other words, the functional $f$ is only a mostly open distribution that excludes $x_0$.
	
	The problem of regularization is as follows: find a distribution over $\mathbb{R}^n$ that is reduced to the $f$ distribution over the open ones that exclude $x_0$. It will be said that this distribution (which extends, in the environment of $x_0$, the definition of the $f$ distribution outside this environment), is a regularization of the $f$ function.
	
\chapter{Elements of Probability}
\label{Appendix_B}
	\begin{definition}
		A probability measure $\mathbb{P}$ on a measurable space $(\Omega, \mathcal{F})$ is a function from $\mathcal{F}$ to $[0, 1]$ such that
			\begin{itemize}
				\item
				$\mathbb{P} (\emptyset) = 0$, and $\mathbb{P} (\Omega) = 1$.
				\item 
				If $\{ A_n \}_{n \geq 1} \in \mathcal{F}$ and $A_i \cap A_j \neq \emptyset$ if $i \neq j$, then $\mathbb{P} (\cup^{\infty}_{n=1} A_n) = \sum^{\infty}_{n = 1} \mathbb{P} (A_n)$.
			\end{itemize}	
	\end{definition}
	
	\noindent A $\sigma$-algebra on a set $X$ is a collection of subsets of $X$ that includes the empty subset and is closed under complement and under countable unions. Denote $\sigma (D) = \cap \{ H : H \text{ is a } \sigma \text{-algebra of } \Omega, D \subseteq H \}$. We call $\sigma (D)$ a $\sigma$-algebra generated by $D$.
	
	\begin{definition}
		A triple  $(\Omega, \mathcal{F}, \mathbb{P})$ is called a probability space if
		\begin{itemize}
			\item
			$\Omega$ is a sample space which is a collection of all samples.
			\item 
			$\mathcal{F}$ is a $\sigma$-algebra on $\Omega$.
			\item
			$\mathbb{P}$ is a probability measure on $(\Omega, \mathcal{F})$. 
		\end{itemize}	
	\end{definition}

	\noindent On a given probability space $(\Omega, \mathcal{F}, \mathbb{P}) (\Omega = \mathbb{R})$, if a cumulative distribution function of a random variable $X$ is normal, i.e.,
	\begin{align}
		\mathbb{P} (X < x) = \displaystyle \int_{-\infty}^{x} \frac{1}{ \sigma \sqrt{2 \pi} } e^{-\frac{(y - \mu)^2}{2 \sigma^2}} dy, \hspace{2mm} \sigma > 0,
	\end{align}
	then the random variable $X$ is called a Gaussian (normal) random variable on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$. Here $X$ is completely characterized by its mean $\mu$ and its standard deviation $\sigma$. We denote $X \sim \mathcal{N} (\mu, \sigma^2)$. The probability density function of $X$ is
	\begin{align*}
		p(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}.
	\end{align*}

	\noindent When $\mu = 0$ and $\sigma = 1$, we call $X$ a standard Gaussian (normal) random variable.
	
	\begin{definition}
		A probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is said to be a complete probability space if for all $B ∈\in \mathcal{F}$ with $\mathbb{P} (B) = 0$ and all $A \subseteq B$ one has $A \in \mathcal{F}$.
	\end{definition}
	
	\begin{definition}
		If $(\Omega, \mathcal{F}, \mathbb{P})$ is a given probability space then a function $Y : \Omega \rightarrow \mathbb{R}^n$ is called $\mathcal{F}$-measurable if $Y^{-1} (U) = \{ w \in \Omega : Y(w) \in U \} \in \mathcal{F}$ holds for all open sets $U \in \mathbb{R}^n$. If $X : \Omega \rightarrow \mathbb{R}^n$ is a function, then $\sigma (X)$ is the smallest $\sigma$-algebra on $\Omega$ containing all the sets $X^{-1} (U)$ for all open sets $U$ in $\mathbb{R}^n$.
	\end{definition}
	
	\begin{definition}
		Suppose that $(\Omega, \mathcal{F}, \mathbb{P})$ is a given complete probability space. A random variable $X$ is an $\mathcal{F}$-measurable function $X : \Omega \rightarrow \mathbb{R}^n$.
	\end{definition}
	
	\noindent Its well known that every random variable induces a probability measure $\mu_X$ (distribution of $X$) on $\mathbb{R}_n$ given by
	\begin{align*}
		\mu_X (B) = \mathbb{P} (X^{-1} (B)).
	\end{align*}
	If $ \int_{\Omega} | X(w) | d \mathbb{P} (w) < \infty$, the expectation of $X$ w.r.t. $\mathbb{P}$ is defined by
	\begin{align*}
		\mathbb{E} [X] = \int_{\Omega} X(w) d \mathbb{P} (w) = \int_{\mathbb{R}^n} x d \mu_X (x).
	\end{align*}  
	Also the $p$-th moment of $X$ is defined as (if the integrals are well defined)
	\begin{align*}
		\mathbb{E} [X^p] = \int_{\Omega} X^p d \mathbb{P} (w) = \int_{\mathbb{R}^n} x^p d \mu_X (x).
	\end{align*}
	The centered moments are defined by $\mathbb{E} \left[ |X - \mathbb{E}[X]| \right]$, $p = 1, 2, \dots, $. When
	$p = 2$, the centered moment is also called the variance. \\
	
	\begin{definition}
		Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and let $T \subseteq \mathbb{R}$ be
		time. A collection of random variables $X_t$ , $t \in T$ with values in $\mathbb{R}$ is called a
		stochastic process. If time is an interval, $\mathbb{R}^+$ or $\mathbb{R}$, it is called a stochastic process with continuous time. For any fixed $w \in \Omega$, one can regard $X_t (w)$ as a function of
		$t$ (called a sample function of the stochastic process).
	\end{definition}

	\noindent On a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a filtration refers to an increasing sequence of $\sigma$-algebras:
	\begin{align*}
		\mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \mathcal{F}_2 \subseteq \cdots \subseteq \mathcal{F}_n \subseteq \cdots .
	\end{align*}
	
	\noindent A natural filtration (w.r.t. $X$) is the smallest $\sigma$-algebra that contains
	information of $X$. It is generated by $X$ and $\mathcal{F}^X_n = \sigma(X_1, \dots, X_n)$ with $\mathcal{F}^X_0 = \{\emptyset, \Omega \}$. If $\lim_{n \rightarrow \infty} \mathcal{F}_n \subseteq \mathcal{F}$, then we call $(\Omega, \mathcal{F}, \{ \mathcal{F}_n \}_{n \geq 1}, \mathbb{P})$ a filtered probability space. A stochastic process $\{X_n \}$ on a filtered probability space is an adapted process if $X_n$ is $\mathcal{F}_n$-measurable for each $n$.
	
	\begin{definition}
		A family of sub-$\sigma$-algebras $\mathcal{F}_t \subseteq \mathcal{F}$ indexed by $t \in [0, \infty)$ is called a filtration if it is increasing $\mathcal{F}_s \subseteq \mathcal{F}_t$ when $0 \leq s \leq t < \infty$.
	\end{definition}
	
	\begin{definition} 
		A collection of random variables is called a Gaussian process, if the joint distribution of any finite number of its members is Gaussian. In other words, a Gaussian process is a $\mathbb{R}^d$-valued stochastic process with continuous time (or with index) $t$ such that $\left(X(t_0), X(t_1), \dots, X(t_n) \right)^T$ is a $(n+1)$-dimensional Gaussian random vector
		for any $0 \leq t_0 < t_1 < \cdots < t_n$. The Gaussian process is denoted as $X = \{X(t)\}_{t \in I}$ where $I$ is a set of indexes.
	\end{definition}
	
	\begin{definition}
		\label{brownian_motion}
		 A continuous time stochastic process $W(t)$ is called a standard Brownian motion if
		\begin{itemize}
			\item
			$W(t)$ is almost surely continuous in $t$, and $W(0) = 0$. 
			\item
			$W(t)$ has independent increments $W(t_{i+1}) - W(t_i)$ for all $t_n \geq 0$, $i = 0, 1, \dots, n$.
			\item
			$W(t) - W(s) \sim \mathcal{N} (0, t-s)$, i.e., obeys the normal distribution with mean zero and variance $t - s$.
		\end{itemize}	
	\end{definition}

	\noindent Set $x \in D \subset \mathbb{R}^d$, we define infinite dimensional Gaussian processes as follows
	\begin{align*}
		W^Q (x, t) = \displaystyle \sum^{\infty}_{j = 1} \sqrt{q_j} e_j (x) W_j (t),
	\end{align*}
	where $W_j (t)$ are mutually independent Brownian motions. Here $q_j \geq 0$, $j \in \mathbb{N}^d$ and $\{ e_j (x) \}$ is an orthonormal basis in $L^2 (D)$.  The following expansion is usually considered in literature:
	\begin{align*}
		\dot{W}^Q (x, t) = \displaystyle \sum^{\infty}_{j = 1} \sqrt{q_j} e_j (x) \dot{W}_j (t).
	\end{align*}
	where $\dot{W}_j (t) =\frac{d}{dt} W$, is formally the first-order derivative of $W_j (t)$ in time. When $q_j = 1$ for all $j$, we have a space-time white noise, and if $\sum^{\infty}_{j = 1} \sqrt{q_j}$ is called a $Q$-Wiener process. \\

	\noindent The Brownian motion and white noise can also be defined in terms of orthogonal expansions. Suppose that $\{e_j (t)\}_{j \geq 1}$ is a complete orthonormal system in $L^2 ([0, T ])$, then the Brownian motion $W(t)$ can be defined by
	\begin{align}
	    W(t) = \displaystyle \sum_{j=1}^{\infty} \beta_j \int_{0}^{t} e_j (s) ds, \hspace{2mm} t \in [0, T],
	\end{align}
	where $\beta_j$ are mutually independent standard Gaussian random variables for each $j$, and it can be also checked that is indeed a standard Brownian motion. Correspondingly, the white noise is defined by
	\begin{align}
	    \dot{W}(t) = \displaystyle \sum_{j=1}^{\infty} \beta_j e_j (t), \hspace{2mm} t \in [0, T].
    \end{align}

	\begin{definition}
		\label{cylindrical}
		Let $\{e_j\}_{j \geq 1}$ be a complete orthonormal  system of a separable Hilbert space $\mathcal{H}$, and $T \in \mathbb{R}^+$, and $\{\beta_j (t)\}_{j \geq 1}$ be an independent and identically distributed sequence of Brownian Motions. Then a cylindrical Wiener process $W$ in $\mathcal{H}$ is given by
		\begin{align*}
			W(t) = \displaystyle \sum_{j=1}^{\infty} \beta_j e_j (t)
		\end{align*}	
	\end{definition}
	
	The Hermite polynomial of grade $n$ evaluated in $\mathbb{R}$ is given as follows
	\begin{align}
		\label{hermite_polynomials}
		P_k(x) = \frac{(-1)^k}{(k!)^{1/2}} e^{\frac{x^2}{2}} \frac{d^k}{dx^k} e^{-\frac{x^2}{2}}
	\end{align}
	with $P_0 = 1$. Is well known that $\{P_k (\dot) \}_{k \in \mathbb{N}}$ is a complete orthonormal system for $L_2 (\mathbb{R}, \mu_1 (dx))$ with $\mu_1 (dx) = \frac{1}{\sqrt{2 \ pi}} e^{- \frac{x^2}{2}} dx$.\\
	
	Define $\Lambda$ as the nuclear covariance operator and Hermite functional given by as follows
	\begin{align}
	\label{hermite_funcionals}
		H_n(h) = \prod_{i=1}^{\infty} P_{n_i} (l_i (h)), h \in \mathcal{H}_0 , n \in J
	\end{align}
	where
	\begin{equation*}
		l_i (h) = ( h, \Lambda^{-1/2} \varphi_i )_{\mathcal{H}}, \hspace{3mm} i = 1,2 \cdots
	\end{equation*}
	and $\mathcal{J}$ is given by
	\begin{align}
		\mathcal{J} = \{\alpha = (\alpha_i,i \geq 1) | \alpha_i \in \mathbb{N}\cup {0}, |\alpha|:= \displaystyle \sum _{i = 0}^{\infty}\alpha_i < \infty\}
	\end{align}
	
	Then we have the following result (see \cite{DAPRATO1994})
	\begin{lemma}
	\label{dense}	
		For $h \in \mathcal{H}$ let $l_i (h) = (h, \Lambda^{-1/2} \varphi_i )_{\mathcal{H}}$, $i=1, 2, \cdots$. Then the set $\{H_n \}$ of all Hermite polynomials on $\mathcal{H}$ forms a complete orthonormal system for $\mathbb{H}$. Hence the set of all functionals are dense in $\mathbb{H}$. Moreover, we have the direct sum decomposition:
		\begin{align*}
		\mathbb{H} = \displaystyle \bigoplus^{\infty}_{j=0} K_j,
		\end{align*}
		where $K_j$ is the subspace of $\mathbb{H}$ spanned by $\{H_n : |n| = j \}$.
	\end{lemma}

	Hence we can the followin decomposition given by 
	\begin{align}
		u_N (t, x) = \displaystyle \sum _{n \in \mathcal{J}^{M, N}} u_{n}(t) H_n (x), \hspace{0.1cm} x \in \mathcal{H}, \hspace{0.1cm} t \in [0, T]
	\end{align}    
	which is known as the deterministic Wiener-Chaos descomposition. 
	
	We define the inner product as follows
	\begin{align*}
		\langle g, h \rangle_0 = \langle \Lambda^{-1/2} g, \Lambda^{-1/2} h \rangle_{\mathcal{H}}, \hspace{2mm} \text{for} \hspace{2mm} g, h \in \Lambda^{-1/2} \mathcal{H}.
	\end{align*}
	
	Set $\mathcal{H}_0$ denote the Hilbert subspace of $\mathcal{H}$, which is the completion of $\Lambda^{-1/2} \mathcal{H}$ with respect to the norm $\| g \|_0 = (g, g)_0^{1/2}$. Then $\mathcal{H}_0$ is dense in $\mathcal{H}$ and the inclusion map $i: \mathcal{H}_0 \longrightarrow \mathcal{H}$ is compact. The triple $(i, \mathcal{H}_0 , \mathcal{H})$ forms an abstract Wiener space. 
	
	Let $\mathbb{H} = L_2 (\mathcal{H}, \mu)$ denote the Hilbert space of Borel measurable functionals on the probability space with inner product
	\begin{align}
		\langle \Phi, \Psi \rangle_{\mathbb{H}} = \displaystyle \int_{\mathcal{H}} \Phi (v) \Psi (v) \mu (dx), \hspace{2mm} \Phi, \Psi \in \mathbb{H},
	\end{align}
	and the norm $\| \Phi \|_{\mathbb{H}} = \langle \Phi, \Phi\rangle_{\mathbb{H}}^{1/2}$. In $\mathcal{H}$ we choose a basis system $\{\varphi_k \}$ such that $\varphi_k \in \mathcal{H}$.\\
	
	A functional $\Phi: \mathcal{H} \longrightarrow  \mathbb{R}$, is said to be a smooth simple functional (or a cylinder functional) if there exists a $C^{\infty} $-function $\varphi$ on $\mathbb{R}_n$ and $n$-continuous linear functional $l_1 ,\cdots ,l_n$ on $\mathcal{H}$ such that for $h \in \mathcal{H}$
	\begin{align*}
		\Phi(h) = \phi(h_1, \cdots, h_n) \hspace{2mm} \text{where} \hspace{2mm} h_i = l_i (h), \hspace{2mm} i = 1, \cdots, n.
	\end{align*}	